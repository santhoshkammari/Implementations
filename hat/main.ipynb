{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da03ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76202103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\santh\\MAIN\\Implementations\\karpathy-series\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1413\n",
      "Input length: 32\n",
      "Input: ['One', 'day,', 'a', 'little', 'girl', 'named', 'Lily', 'found', 'a', 'needle']...\n",
      "Target: 'with'\n",
      "\n",
      "Number of batches: 353\n",
      "Batch inputs type: <class 'list'>\n",
      "Batch inputs length: 4 samples\n",
      "Each input length: 32 words\n",
      "Batch targets: ['for', 'a', 'All', 'their']\n",
      "First input: ['they', 'shared', 'the', 'needle', 'and']...\n",
      "First target: 'for'\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset as HfDataset\n",
    "\n",
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, name, sample=None, context_size=32):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        # Load data\n",
    "        self.train = HfDataset.from_parquet(name)\n",
    "        if sample:\n",
    "            self.train = self.train.select(range(sample))\n",
    "        \n",
    "        # Split into words\n",
    "        self._texts = \"\\n\".join(self.train['text']).split()\n",
    "        \n",
    "        # Create sliding windows of size context_size + 1\n",
    "        # We need +1 because we'll split into input and target\n",
    "        self.data = [self._texts[idx:idx+context_size+1]\n",
    "                     for idx in range(len(self._texts) - context_size)]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sequence = self.data[index]  # This has context_size + 1 tokens\n",
    "        \n",
    "        # Split into input and target\n",
    "        inputs = sequence[:-1]   # First context_size tokens\n",
    "        target = sequence[-1]    # Last token (what we want to predict)\n",
    "        \n",
    "        return inputs, target\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    inputs = [item[0] for item in batch]  # List of input sequences\n",
    "    targets = [item[1] for item in batch] # List of target tokens\n",
    "    return inputs, targets\n",
    "\n",
    "# Usage\n",
    "train_dataset = StoryDataset('train.parquet', sample=10, context_size=32)\n",
    "print(f\"Dataset length: {len(train_dataset)}\")\n",
    "\n",
    "# Test individual sample\n",
    "inputs, target = train_dataset[0]\n",
    "print(f\"Input length: {len(inputs)}\")\n",
    "print(f\"Input: {inputs[:10]}...\")  # First 10 words\n",
    "print(f\"Target: '{target}'\")\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\nNumber of batches: {len(dataloader)}\")\n",
    "for batch_inputs, batch_targets in dataloader:\n",
    "    print(f\"Batch inputs type: {type(batch_inputs)}\")\n",
    "    print(f\"Batch inputs length: {len(batch_inputs)} samples\")\n",
    "    print(f\"Each input length: {len(batch_inputs[0])} words\")\n",
    "    print(f\"Batch targets: {batch_targets}\")\n",
    "    print(f\"First input: {batch_inputs[0][:5]}...\")  # First 5 words\n",
    "    print(f\"First target: '{batch_targets[0]}'\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAT(nn.Module):\n",
    "    \"\"\"\n",
    "    charencoder\n",
    "    backbone\n",
    "    chardecoder\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615d0f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[W]', 'f', 'o', 'r', ' ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'for'\n",
    "chars = [x for x in word]\n",
    "chars = ['[W]'] + chars + [' ']\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a69320",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embed = []\n",
    "for ch in chars:\n",
    "    ar = [0]*256\n",
    "    for _ in ch.encode('utf-8'):\n",
    "        ar[_]=1\n",
    "    char_embed.append(ar)\n",
    "\n",
    "char_emb = torch.tensor(char_embed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "110f0b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5527701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_embed(char_embed):\n",
    "    nn.cross_atte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed = self_attend(char_embed)[0,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
