{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24d98859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class DualBertCrossAttention(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', hidden_size=768, freeze_input_bert=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Dual BERT setup\n",
    "        self.input_bert = BertModel.from_pretrained(model_name)\n",
    "        self.class_bert = BertModel.from_pretrained(model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Freezing strategy\n",
    "        if freeze_input_bert:\n",
    "            for param in self.input_bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Cross-attention layers\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def tokenize_inputs(self, texts: List[str], max_length=128) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Tokenize input texts with padding and attention masks.\"\"\"\n",
    "        encoding = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoding\n",
    "    \n",
    "    def tokenize_classes(self, all_classes: List[List[str]], max_class_length=32) -> Tuple[torch.Tensor, torch.Tensor, List[List[Tuple[int, int]]]]:\n",
    "        \"\"\"\n",
    "        Tokenize class labels with padding and track boundaries.\n",
    "        \n",
    "        Args:\n",
    "            all_classes: List of class lists per sample [[\"very good\", \"bad\"], [\"positive\", \"negative\", \"neutral\"]]\n",
    "            max_class_length: Max tokens per individual class\n",
    "            \n",
    "        Returns:\n",
    "            class_input_ids: [batch_size, max_total_class_tokens]\n",
    "            class_attention_mask: [batch_size, max_total_class_tokens] \n",
    "            class_boundaries: [batch_size][class_idx] = (start, end) positions\n",
    "        \"\"\"\n",
    "        batch_size = len(all_classes)\n",
    "        batch_class_tokens = []\n",
    "        batch_boundaries = []\n",
    "        \n",
    "        for sample_classes in all_classes:\n",
    "            sample_tokens = []\n",
    "            sample_boundaries = []\n",
    "            current_pos = 0\n",
    "            \n",
    "            # Tokenize each class in this sample\n",
    "            for class_text in sample_classes:\n",
    "                # Tokenize single class (remove [CLS], [SEP])\n",
    "                tokens = self.tokenizer(\n",
    "                    class_text,\n",
    "                    add_special_tokens=False,\n",
    "                    max_length=max_class_length,\n",
    "                    truncation=True\n",
    "                )['input_ids']\n",
    "                \n",
    "                # Record boundaries\n",
    "                start_pos = current_pos\n",
    "                end_pos = current_pos + len(tokens)\n",
    "                sample_boundaries.append((start_pos, end_pos))\n",
    "                \n",
    "                sample_tokens.extend(tokens)\n",
    "                current_pos = end_pos\n",
    "            \n",
    "            batch_class_tokens.append(sample_tokens)\n",
    "            batch_boundaries.append(sample_boundaries)\n",
    "        \n",
    "        # Pad all samples to same length\n",
    "        max_total_tokens = max(len(tokens) for tokens in batch_class_tokens)\n",
    "        \n",
    "        padded_input_ids = []\n",
    "        attention_masks = []\n",
    "        \n",
    "        for tokens in batch_class_tokens:\n",
    "            # Pad tokens\n",
    "            pad_length = max_total_tokens - len(tokens)\n",
    "            padded_tokens = tokens + [self.tokenizer.pad_token_id] * pad_length\n",
    "            \n",
    "            # Create attention mask (1 for real tokens, 0 for padding)\n",
    "            attention_mask = [1] * len(tokens) + [0] * pad_length\n",
    "            \n",
    "            padded_input_ids.append(padded_tokens)\n",
    "            attention_masks.append(attention_mask)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(padded_input_ids),\n",
    "            torch.tensor(attention_masks),\n",
    "            batch_boundaries\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_texts: List[str], class_lists: List[List[str]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with cross-attention between input and class tokens.\n",
    "        \n",
    "        Args:\n",
    "            input_texts: [\"how are you\", \"i am good\"] \n",
    "            class_lists: [[\"positive\", \"negative\"], [\"good\", \"bad\", \"neutral\"]]\n",
    "            \n",
    "        Returns:\n",
    "            logits: [batch_size, max_classes] - padded with -inf for missing classes\n",
    "        \"\"\"\n",
    "        batch_size = len(input_texts)\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        # 1. Tokenize inputs\n",
    "        input_encoding = self.tokenize_inputs(input_texts)\n",
    "        input_ids = input_encoding['input_ids'].to(device)\n",
    "        input_attention_mask = input_encoding['attention_mask'].to(device)\n",
    "        \n",
    "        # 2. Tokenize classes  \n",
    "        class_ids, class_attention_mask, class_boundaries = self.tokenize_classes(class_lists)\n",
    "        class_ids = class_ids.to(device)\n",
    "        class_attention_mask = class_attention_mask.to(device)\n",
    "        \n",
    "        # 3. Get BERT embeddings\n",
    "        with torch.no_grad() if self.input_bert.training == False else torch.enable_grad():\n",
    "            input_outputs = self.input_bert(input_ids, attention_mask=input_attention_mask)\n",
    "            input_embeddings = input_outputs.last_hidden_state  # [batch_size, input_seq_len, hidden_size]\n",
    "        \n",
    "        class_outputs = self.class_bert(class_ids, attention_mask=class_attention_mask)\n",
    "        class_embeddings = class_outputs.last_hidden_state  # [batch_size, class_seq_len, hidden_size]\n",
    "        \n",
    "        # 4. Global Cross-Attention: Each token type attends to the other\n",
    "        # Input tokens attend to class tokens\n",
    "        input_enhanced, _ = self.cross_attention(\n",
    "            query=input_embeddings,\n",
    "            key=class_embeddings, \n",
    "            value=class_embeddings,\n",
    "            key_padding_mask=~class_attention_mask.bool()  # True for padding positions\n",
    "        )\n",
    "        \n",
    "        # Class tokens attend to input tokens  \n",
    "        class_enhanced, _ = self.cross_attention(\n",
    "            query=class_embeddings,\n",
    "            key=input_embeddings,\n",
    "            value=input_embeddings, \n",
    "            key_padding_mask=~input_attention_mask.bool()\n",
    "        )\n",
    "        \n",
    "        # 5. Pool class tokens back to class representations\n",
    "        batch_logits = []\n",
    "        max_classes = max(len(classes) for classes in class_lists)\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            sample_class_reps = []\n",
    "            boundaries = class_boundaries[batch_idx]\n",
    "            \n",
    "            # Extract representation for each class\n",
    "            for start, end in boundaries:\n",
    "                # Mean pool tokens belonging to this class\n",
    "                class_tokens = class_enhanced[batch_idx, start:end, :]  # [class_len, hidden_size]\n",
    "                class_rep = class_tokens.mean(dim=0)  # [hidden_size]\n",
    "                sample_class_reps.append(class_rep)\n",
    "            \n",
    "            # Convert to logits\n",
    "            sample_class_reps = torch.stack(sample_class_reps)  # [num_classes, hidden_size]\n",
    "            sample_logits = self.classifier(self.dropout(sample_class_reps)).squeeze(-1)  # [num_classes]\n",
    "            \n",
    "            # Pad to max_classes for batch consistency\n",
    "            if len(sample_logits) < max_classes:\n",
    "                padding = torch.full((max_classes - len(sample_logits),), float('-inf'), device=device)\n",
    "                sample_logits = torch.cat([sample_logits, padding])\n",
    "            \n",
    "            batch_logits.append(sample_logits)\n",
    "        \n",
    "        return torch.stack(batch_logits)  # [batch_size, max_classes]\n",
    "\n",
    "\n",
    "    def inference(self, input_texts: List[str], class_lists: List[List[str]]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Inference function that returns predictions with probabilities.\n",
    "        \n",
    "        Args:\n",
    "            input_texts: List of input texts to classify\n",
    "            class_lists: List of class options for each input\n",
    "            \n",
    "        Returns:\n",
    "            List of prediction dictionaries with probabilities and predicted class\n",
    "        \"\"\"\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get logits from forward pass\n",
    "            logits = self.forward(input_texts, class_lists)\n",
    "            \n",
    "            predictions = []\n",
    "            \n",
    "            for i, (sample_logits, classes) in enumerate(zip(logits, class_lists)):\n",
    "                num_classes = len(classes)\n",
    "                \n",
    "                # Extract valid logits (remove padding)\n",
    "                valid_logits = sample_logits[:num_classes]\n",
    "                \n",
    "                # Convert to probabilities\n",
    "                probabilities = F.softmax(valid_logits, dim=0)\n",
    "                \n",
    "                # Get prediction\n",
    "                pred_idx = torch.argmax(valid_logits).item()\n",
    "                pred_class = classes[pred_idx]\n",
    "                pred_prob = probabilities[pred_idx].item()\n",
    "                \n",
    "                # Create result dictionary\n",
    "                result = {\n",
    "                    'input_text': input_texts[i],\n",
    "                    'predicted_class': pred_class,\n",
    "                    'predicted_index': pred_idx,\n",
    "                    'confidence': pred_prob,\n",
    "                    'all_probabilities': {\n",
    "                        class_name: prob.item() \n",
    "                        for class_name, prob in zip(classes, probabilities)\n",
    "                    },\n",
    "                    'class_options': classes\n",
    "                }\n",
    "                \n",
    "                predictions.append(result)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7d48df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DualBertCrossAttention(\n",
       "  (input_bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (class_bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cross_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model = DualBertCrossAttention(freeze_input_bert=True,hidden_size=128)\n",
    " model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bba89626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 219,030,657\n",
      "In Millions (M): 219.03M\n",
      "In Billions (B): 0.219B\n"
     ]
    }
   ],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Convert to M and B\n",
    "params_in_m = total_params / 1e6\n",
    "params_in_b = total_params / 1e9\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"In Millions (M): {params_in_m:.2f}M\")\n",
    "print(f\"In Billions (B): {params_in_b:.3f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbfafd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([3, 3])\n",
      "Average loss: 0.8419\n",
      "Sample 0: Predicted 'quite negative' (index 1)\n",
      "Sample 1: Predicted 'neutral' (index 2)\n",
      "Sample 2: Predicted 'negative' (index 1)\n"
     ]
    }
   ],
   "source": [
    "# Usage Example\n",
    "def train_example():\n",
    "    # Sample data\n",
    "    input_texts = [\n",
    "        \"how are you doing today\",\n",
    "        \"i am feeling great\", \n",
    "        \"this movie was okay\"\n",
    "    ]\n",
    "    \n",
    "    class_lists = [\n",
    "        [\"very positive\", \"quite negative\"],  # 2 classes\n",
    "        [\"good\", \"bad\", \"neutral\"],           # 3 classes  \n",
    "        [\"positive\", \"negative\"]              # 2 classes\n",
    "    ]\n",
    "    \n",
    "    # Ground truth labels (indices within each sample's classes)\n",
    "    targets = [1, 0, 0]  # \"quite negative\", \"good\", \"positive\"\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DualBertCrossAttention(freeze_input_bert=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(input_texts, class_lists)\n",
    "    print(f\"Logits shape: {logits.shape}\")  # [3, 3] (padded to max_classes=3)\n",
    "    \n",
    "    # Compute loss (handle variable class numbers)\n",
    "    total_loss = 0\n",
    "    for i, (sample_logits, target_idx) in enumerate(zip(logits, targets)):\n",
    "        num_classes = len(class_lists[i])\n",
    "        valid_logits = sample_logits[:num_classes]  # Remove padded positions\n",
    "        loss = F.cross_entropy(valid_logits.unsqueeze(0), torch.tensor([target_idx]))\n",
    "        total_loss += loss\n",
    "    \n",
    "    avg_loss = total_loss / len(targets)\n",
    "    print(f\"Average loss: {avg_loss.item():.4f}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    avg_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Predictions\n",
    "    predictions = []\n",
    "    for i, sample_logits in enumerate(logits):\n",
    "        num_classes = len(class_lists[i])\n",
    "        valid_logits = sample_logits[:num_classes]\n",
    "        pred_idx = torch.argmax(valid_logits).item()\n",
    "        pred_class = class_lists[i][pred_idx]\n",
    "        predictions.append((pred_idx, pred_class))\n",
    "        print(f\"Sample {i}: Predicted '{pred_class}' (index {pred_idx})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8118aac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample 1 ---\n",
      "Input: 'I absolutely love this product!'\n",
      "Predicted: 'negative' (confidence: 0.200)\n",
      "All probabilities:\n",
      "  very positive: 0.200\n",
      "  positive: 0.200\n",
      "  neutral: 0.200\n",
      "  negative: 0.200\n",
      "  very negative: 0.200\n",
      "\n",
      "--- Sample 2 ---\n",
      "Input: 'The weather is okay today'\n",
      "Predicted: 'good' (confidence: 0.250)\n",
      "All probabilities:\n",
      "  excellent: 0.249\n",
      "  good: 0.250\n",
      "  average: 0.250\n",
      "  poor: 0.250\n",
      "\n",
      "--- Sample 3 ---\n",
      "Input: 'This service was terrible'\n",
      "Predicted: 'dissatisfied' (confidence: 0.334)\n",
      "All probabilities:\n",
      "  satisfied: 0.333\n",
      "  dissatisfied: 0.334\n",
      "  neutral: 0.333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage example:\n",
    "def run_inference():\n",
    "    # Load trained model\n",
    "    model = DualBertCrossAttention(freeze_input_bert=True)\n",
    "    # model.load_state_dict(torch.load('trained_model.pth'))  # Load trained weights\n",
    "    \n",
    "    # Sample inputs for inference\n",
    "    test_inputs = [\n",
    "        \"I absolutely love this product!\",\n",
    "        \"The weather is okay today\", \n",
    "        \"This service was terrible\"\n",
    "    ]\n",
    "    \n",
    "    test_classes = [\n",
    "        [\"very positive\", \"positive\", \"neutral\", \"negative\", \"very negative\"],\n",
    "        [\"excellent\", \"good\", \"average\", \"poor\"],\n",
    "        [\"satisfied\", \"dissatisfied\", \"neutral\"]\n",
    "    ]\n",
    "    \n",
    "    # Run inference\n",
    "    results = model.inference(test_inputs, test_classes)\n",
    "    \n",
    "    # Print results\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"Input: '{result['input_text']}'\")\n",
    "        print(f\"Predicted: '{result['predicted_class']}' (confidence: {result['confidence']:.3f})\")\n",
    "        print(f\"All probabilities:\")\n",
    "        for class_name, prob in result['all_probabilities'].items():\n",
    "            print(f\"  {class_name}: {prob:.3f}\")\n",
    "\n",
    "run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
